# Basic model with conitnual learning tried out. 
PTQ and unstructured pruning (50%) still used, continuing from baseline_0, 
but an adapter head was added. We freeze the quantized backbone and attach a tiny float-precision “adapter head”  
(a 128→32 down + ReLU + 32→10 output) that is trainable. The metrics between the FP32 vs pruned vs quantized vs hybrid
can be compared. Continual learning using avalanche is implemnted (5-split MNIST stream) comparing both a naïve
rehearsal-free strategy and a replay buffer. I am interested in 1: batch size effects on learning because currently 
a high batch size is needed to avoid CF, as well as 2: peak SRAM usage because it is too high


